{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numba import *\n",
    "from numba import cuda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cuda_indexing.png\" width=\"480\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cuda_indexing_0.png\" width=\"480\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid > Block > Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/memory-hierarchy.png\" width=\"240\" height=\"240\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - thread ID of a thread of index (x, y) is (x + y Dx)\n",
    "    - the size of a block should be a multiple of 32 threads\n",
    "        a) typical size is between 128 and 512 threads\n",
    "    - 32 thread form a warp(?)\n",
    "    -  A warp is a set of 32 threads within a thread block such that all the threads in a warp execute the same instruction\n",
    "    \n",
    "    - The fastest memory is registers just as in CPU. \n",
    "    - L1 cache and shared memory is second, which is also pretty limited in size.\n",
    "    - Individual threads composing a warp start together at the same program address, but they have their own instruction address counter and register state and are therefore free to branch and execute independently.\n",
    "    - Each warp gets scheduled by a warp scheduler for execution\n",
    "    - Each warp contains threads of consecutive, increasing thread IDs with the first warp containing thread 0.\n",
    "    - A warp executes one common instruction at a time, so full efficiency is realized when all 32 threads of a warp agree on their execution path.\n",
    "    - If threads of a warp diverge via a data-dependent conditional branch, the warp executes each branch path taken, disabling threads that are not on that path\n",
    "    - The threads of a warp that are participating in the current instruction are called the active threads, whereas threads not on the current instruction are inactive (disabled). \n",
    "    - If an atomic instruction executed by a warp reads, modifies, and writes to the same location in global memory for more than one of the threads of the warp, each read/modify/write to that location occurs and they are all serialized, but the order in which they occur is undefined. \n",
    "    - The execution context (program counters, registers, etc.) for each warp processed by a multiprocessor is maintained on-chip during the entire lifetime of the warp. Therefore, switching from one execution context to another has no cost, and at every instruction issue time, a warp scheduler selects a warp that has threads ready to execute its next instruction (the active threads of the warp) and issues the instruction to those threads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The number of blocks and warps that can reside and be processed together on the multiprocessor for a given kernel depends on the amount of registers and shared memory used by the kernel and the amount of registers and shared memory available on the multiprocessor.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Warp Schedulers means two (4 for GTX 1050 ti?) warps can be issued at the same time. 32*4*6 = 768 threads per SM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>threads of a thread block execute concurrently on one multiprocessor</b>, and <b>multiple thread blocks can execute concurrently on one multiprocessor</b>. As thread blocks terminate, new blocks are launched on the vacated multiprocessors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1050 ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 128 single-precision CUDA cores per SM, GeForce GTX 1050 Ti wields a total of 768. Each SM ( of <b>6 SM </b>) also includes eight texture units (adding up to 48 across the processor), 256 KB of register file capacity, 96 KB of shared memory, and 48 KB of L1/texture cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>L1, L2, L3 caches</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кэш память первого уровня L1 — самая маленькая по объему (всего несколько десятков килобайт), но самая быстрая по скорости и наиболее важная. Она содержит данные наиболее часто используемые процессором и работает без задержек. Обычно количество микросхем памяти уровня L1 равно количеству ядер процессора, при этом каждое ядро получает доступ только к своей микросхеме L1.\n",
    "\n",
    "Кэш память уровня L2 по скорости уступает памяти L1, но выигрывает в объеме, который измеряется уже в нескольких сотнях килобайт. Она предназначена для временного хранения важной информации, вероятность обращения к которой ниже, чем у информации хранящейся в кэше L1.\n",
    "\n",
    "Третий уровень кэш памяти L3 — имеет самый большой объем из трех уровней (может достигать десятков мегабайт), но и обладает самой медленной скоростью, которая всё же значительно выше скорости оперативной памяти. Кэш память L3 служит общей для всех ядер процессора. Уровень памяти L3 предназначен для временного хранения тех важных данных, вероятность обращения к которым чуть ниже, чем у информации которая хранится в первых двух уровнях L1, L2. Она также обеспечивает взаимодействие ядер процессора между собой. Некоторые модели процессоров выполнены с двумя уровнями кэш памяти, в которых L2 совмещает все функции L2 и L3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - global device memory\n",
    "    - shared memory\n",
    "    - local memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the software side, the block size determines how many threads share a given area of shared memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block size you choose depends on a range of factors, including:\n",
    "\n",
    "    The size of the data array\n",
    "    The size of the shared mempory per block (e.g. 64KB)\n",
    "    The maximum number of threads per block supported by the hardware (e.g. 512 or 1024)\n",
    "    The maximum number of threads per multiprocessor (MP) (e.g. 2048)\n",
    "    The maximum number of blocks per MP (e.g. 32)\n",
    "    The number of threads that can be executed concurrently (a “warp” i.e. 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA recommends that programmers focus on following those recommendations to achieve the best performance:\n",
    "\n",
    "    Find ways to parallelize sequential code\n",
    "    Minimize data transfers between the host and the device\n",
    "    Adjust kernel launch configuration to maximize device utilization\n",
    "    Ensure global memory accesses are coalesced\n",
    "    Minimize redundant accesses to global memory whenever possible\n",
    "    Avoid different execution paths within the same warp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <b>Kernel</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kernel function is a GPU function that is meant to be called from CPU code. It has two fundamental characteristics:\n",
    "\n",
    "    kernels cannot explicitly return a value; all result data must be written to an array passed to the function (if computing a scalar, you will probably pass a one-element array);\n",
    "    kernels explicitly declare their thread hierarchy when called: i.e. the number of thread blocks and the number of threads per block (note that while a kernel is compiled once, it can be called multiple times with different block sizes or grid sizes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of registers per thread that kernel used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.is_available()\n",
    "#cuda.get_memory_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994201512378423\n",
      "0.9998168949194969\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "@cuda.jit\n",
    "def max_example(result, values):\n",
    "#   Find the maximum value in values and store in result[0]\n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    bdim = cuda.blockDim.x\n",
    "    i = (bid * bdim) + tid\n",
    "    cuda.atomic.max(result, 0, values[i])\n",
    "\n",
    "\n",
    "arr = np.random.rand(16384)\n",
    "result = np.zeros(1, dtype=np.float64)\n",
    "\n",
    "# Max # of resident threads per SM is 2048 for 1050 ti !!!\n",
    "# Max # of threads per block 1024 !!!\n",
    "max_example[4,1024](result, arr) \n",
    "print(result[0]) # Found using cuda.atomic.max\n",
    "print(max(arr))  # Print max(arr) for comparision (should be equal!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=7iE0sHiGGxs Occupancy limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example of parameters calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many thread blocks per SM can we run?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For some GPU, Limit is hardware one\n",
    "\n",
    "                     Kernel                          Limit (per SM)\n",
    "\n",
    "Threads               1024                         |   2048         => No more than 2 blocks  --- MAIN LIMIT\n",
    "\n",
    "Registers             7 per thread (7168 in total) |   65536        => No more than 9 blocks\n",
    "\n",
    "Shared memory         4kB                          |   48kB         => 12 threadblocks    \n",
    "\n",
    "#of blocks                                         |      8         => 8 blocks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "numba.cuda.threadIdx - The thread indices in the current thread block. For 1-dimensional blocks, the index (given by the x attribute) is an integer spanning the range from 0 to numba.cuda.blockDim - 1. A similar rule exists for each dimension when more than one dimension is used.\n",
    "numba.cuda.blockDim - The shape of the block of threads, as declared when instantiating the kernel. This value is the same for all threads in a given kernel, even if they belong to different blocks (i.e. each block is “full”).\n",
    "numba.cuda.blockIdx - The block indices in the grid of threads launched a kernel. For a 1-dimensional grid, the index (given by the x attribute) is an integer spanning the range from 0 to numba.cuda.gridDim - 1. A similar rule exists for each dimension when more than one dimension is used.\n",
    "numba.cuda.gridDim - The shape of the grid of blocks, i.e. the total number of blocks launched by this kernel invocation, as declared when instantiating the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 dim grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    \n",
    "    #Return the absolute position of the current thread in the entire grid of blocks.\n",
    "    pos = cuda.grid(1) \n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] *= 2 # do the computation\n",
    "\n",
    "# Host code   \n",
    "data = np.ones(256)\n",
    "threadsperblock = 256\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 dim grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def my_kernel_2D(io_array):\n",
    "    x, y = cuda.grid(2)\n",
    "    if x < io_array.shape[0] and y < io_array.shape[1]:\n",
    "        io_array[x,y] *= 2 # do the computation\n",
    "\n",
    "data= np.ones((16, 16))\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = math.ceil(data.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(data.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "my_kernel_2D[blockspergrid, threadsperblock](data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numba.cuda.cudadrv.devicearray.DeviceNDArray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_array = cuda.device_array(2)\n",
    "type(device_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numba.cuda.cudadrv.devicearray.DeviceNDArray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_array = cuda.to_device([1,2,3])\n",
    "type(device_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example Matrix multipliction</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "        \n",
    "# Host code\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = numpy.full((24, 12), 3, numpy.float) # matrix containing all 3's\n",
    "B = numpy.full((12, 22), 4, numpy.float) # matrix containing all 4's\n",
    "\n",
    "# Copy the arrays to the device\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "\n",
    "# Allocate memory on the device for the result\n",
    "C_global_mem = cuda.device_array((24, 22))\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "\n",
    "# Copy the result back to the host\n",
    "C = C_global_mem.copy_to_host()\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! A problem with this code is that each thread is <b>reading from the global memory</b> containing the copies of A and B.\n",
    "In fact, the A global memory is read B.shape[1] times and the B global memory is read A.shape[0] times. Since global memory is fairly slow, this results in an inefficient use of the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Shared memory and thread synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda.shared.array\n",
    "# cuda.syncthreads() # This function will synchronize all threads in the same thread block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is allocated once for the duration of the kernel, unlike traditional dynamic memory management.\n",
    "\n",
    "Because the shared memory is a limited resource, it is often necessary to preload a small block at a time from the input arrays. All the threads then need to wait until everyone has finished preloading before doing the computation on the shared memory.\n",
    "\n",
    "Synchronization is then required again after the computation to ensure all threads have finished with the data in shared memory before overwriting it in the next loop iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Advanced matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]]\n"
     ]
    }
   ],
   "source": [
    "from numba import float32\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp\n",
    "\n",
    "# The data array\n",
    "A = numpy.full((TPB*2, TPB*3), 3, numpy.float) # [32 x 48] matrix containing all 3's\n",
    "B = numpy.full((TPB*3, TPB*1), 4, numpy.float) # [48 x 16] matrix containing all 4's\n",
    "\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "C_global_mem = cuda.device_array((TPB*2, TPB*1)) # [32 x 16] matrix result\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (TPB, TPB)\n",
    "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[1]))\n",
    "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[0]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "fast_matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "res = C_global_mem.copy_to_host()\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Kernel doesn't require specify type of itself and the arguments</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
