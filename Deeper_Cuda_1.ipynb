{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumed 0.504155 s\n",
      "0.12010806372454963\n",
      "3.0\n",
      "3.1201080637245497\n",
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "# How much memory does it use?\n",
    "N = 1024 # !\n",
    "blockspergrid = 1 # Memory !!! If < #blocks x # threads < matrix shape it does not work yet!!! Why?\n",
    "threadsperblock = 1024 # why number of threads important ?\n",
    "\n",
    "# When types are specified it doesn't operate properly\n",
    "@cuda.jit(max_registers=1)#('void(float32[:,:], float32[:,:], float32[:,:])') # Assign types explicitly\n",
    "def kernel_1(a,b,out):\n",
    "    x = cuda.grid(1)\n",
    "    if x <= out.shape[0]: #and y <= out.shape[1]: # In order not to exceed the shapes?\n",
    "        out[x] = a[x] + b[x]\n",
    "\n",
    "a = cuda.to_device((np.random.rand(N)).astype(np.float64))\n",
    "b = cuda.to_device(3*np.ones(N).astype(np.float64))\n",
    "out = cuda.device_array_like(a) # Empty array\n",
    "\n",
    "start = timer()\n",
    "kernel_1[blockspergrid, threadsperblock](a, b, out)\n",
    "t_ = timer() - start\n",
    "\n",
    "print (\"Time consumed %f s\" % t_)\n",
    "print(a.copy_to_host()[1023])\n",
    "print(b.copy_to_host()[1023])\n",
    "print(out.copy_to_host()[1023])\n",
    "print(out.copy_to_host().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.rand(1)\n",
    "print(sample.nbytes, type(sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "CudaAPIError",
     "evalue": "[201] Call to cuMemAlloc results in CUDA_ERROR_INVALID_CONTEXT",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-d49c98632869>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_array_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Empty array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\env\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py\u001b[0m in \u001b[0;36m_require_cuda_context\u001b[1;34m(*args, **kws)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\env\\lib\\site-packages\\numba\\cuda\\api.py\u001b[0m in \u001b[0;36mto_device\u001b[1;34m(obj, stream, copy, to)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \"\"\"\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mto\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevicearray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\env\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py\u001b[0m in \u001b[0;36mauto_device\u001b[1;34m(obj, stream, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m                 subok=True)\n\u001b[0;32m    685\u001b[0m             \u001b[0msentry_contiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[0mdevobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrom_array_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m             \u001b[0mdevobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_to_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\env\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py\u001b[0m in \u001b[0;36mfrom_array_like\u001b[1;34m(ary, stream, gpu_data)\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[0mary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m     return DeviceNDArray(ary.shape, ary.strides, ary.dtype,\n\u001b[1;32m--> 624\u001b[1;33m                          writeback=ary, stream=stream, gpu_data=gpu_data)\n\u001b[0m\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\env\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, shape, strides, dtype, stream, writeback, gpu_data)\u001b[0m\n\u001b[0;32m    100\u001b[0m                                                                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                                                                 self.dtype.itemsize)\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0mgpu_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemalloc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malloc_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malloc_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_memory_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\env\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py\u001b[0m in \u001b[0;36mmemalloc\u001b[1;34m(self, bytesize)\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuMemAlloc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytesize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attempt_allocation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallocator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[0mfinalizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_alloc_finalizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytesize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\env\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py\u001b[0m in \u001b[0;36m_attempt_allocation\u001b[1;34m(self, allocator)\u001b[0m\n\u001b[0;32m    678\u001b[0m         \"\"\"\n\u001b[0;32m    679\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             \u001b[0mallocator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCudaAPIError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[1;31m# is out-of-memory?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\env\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py\u001b[0m in \u001b[0;36mallocator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mallocator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 695\u001b[1;33m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuMemAlloc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytesize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attempt_allocation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallocator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\env\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py\u001b[0m in \u001b[0;36msafe_cuda_api_call\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'call driver api: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[0mretcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msafe_cuda_api_call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\env\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py\u001b[0m in \u001b[0;36m_check_error\u001b[1;34m(self, fname, retcode)\u001b[0m\n\u001b[0;32m    323\u001b[0m                     \u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_getpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mCudaDriverError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CUDA initialized before forking\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mCudaAPIError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCudaAPIError\u001b[0m: [201] Call to cuMemAlloc results in CUDA_ERROR_INVALID_CONTEXT"
     ]
    }
   ],
   "source": [
    "# How much memory does it use?\n",
    "N = 1\n",
    "blockspergrid = 1 # Memory !!! If < #blocks x # threads < matrix shape it does not work yet!!! Why?\n",
    "threadsperblock = 2 # why number of threads important ?\n",
    "\n",
    "# When types are specified it doesn't operate properly\n",
    "@cuda.jit#('void(float32[:,:], float32[:,:], float32[:,:])') # Assign types explicitly\n",
    "def kernel_1(a,b,out):\n",
    "    x = cuda.grid(1)\n",
    "    if x < 1: #and y <= out.shape[1]: # In order not to exceed the shapes?\n",
    "        for j in range(out.shape[0]):\n",
    "            out[j] = a[j] + b[j]\n",
    "        \n",
    "a = cuda.to_device(3*np.ones((N), dtype=np.float64))\n",
    "b = cuda.to_device(3*np.ones((N),dtype=np.float64))\n",
    "out = cuda.device_array_like(a) # Empty array\n",
    "\n",
    "start = timer()\n",
    "kernel_1[blockspergrid, threadsperblock](a, b, out)\n",
    "t_ = timer() - start\n",
    "\n",
    "print (\"Time consumed %f s\" % t_)\n",
    "print(a.copy_to_host())\n",
    "print(b.copy_to_host())\n",
    "print(out.copy_to_host())\n",
    "print(out.copy_to_host().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(np.array([1.]).dtype)\n",
    "print(np.arange(10).nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. The global, constant, and texture memory spaces are optimized for different memory usages (see Device Memory Accesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of shared memory is when you need to within a block of threads, reuse data already pulled or evaluated from global memory. So instead of pulling from global memory again, you put it in the shared memory for other threads within the same block to see and reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global memory 8GB?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cudaMalloc` always allocates global memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `double` variable and each `long long` variable uses `two registers`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SM 10 registers per thread\n",
    "and \n",
    "If each thread uses 11 registers -> threads per SM are reduced "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local memory is private for each thread it is not memory it is kept in global memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the variables that can't be stored in registers because there is lack of them goes to the local memory which is a part of global device memory and provides high memory latency in contrast to registers. This is called register spilling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important to try to keep all the variables in registers. The impact of register spilling is often underestimated by new Cuda developers. I made some tests in which I artificially doubled the amount of memory used by threads and caused register spilling without any other computation costs and it increased the time of computations 5 times! In small CUDA applications the number of registers is enough. You can find out how many variables goes to local memory by following the instruction in the pdf above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_MemoryInfo(free=3530332569, total=4294967296)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.cudadrv.driver.Context.get_memory_info(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'GeForce GTX 1050 Ti'                              [SUPPORTED]\n",
      "                      compute capability: 6.1\n",
      "                           pci device id: 0\n",
      "                              pci bus id: 1\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numba.cuda.cudadrv.devices._DeviceList at 0x21750827898>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CUDA context c_void_p(2299135068240) of device 0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.current_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numba.cuda.cudadrv.driver.Context"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.cudadrv.driver.Context #?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'GeForce GTX 1050 Ti'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.cudadrv.driver.Device(0).name#.compute_capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.cudadrv.driver.Device(0).compute_capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Float type review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Clarifying images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/image032.jpg\" width=\"480\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/image034.jpg\" width=\"480\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "See the pic above\n",
    "\n",
    "Algorithm to get from 6 to -6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Достаточно просто копировать исходную комбинацию спра­ва налево до тех пор, пока не будет встречена единица, а затем последовательно заменять значения оставшихся битов их дополнениями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/image036.jpg\" width=\"480\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>IEEE 754</b>:\n",
    "       \n",
    "    —Single precision numbers include an 8-bit exponent field and a 23-bit fraction, for a total of 32bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\pm mantissa*{2^{\\exp }}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/norm_repr.png\" width=\"480\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/mant.png\" width=\"480\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are special cases that require encodings:\n",
    "\n",
    "    – Infinities (overflow)\n",
    "    – NAN (divide by zero)\n",
    "    For example:\n",
    "        – Single-precision: 8 bits in e → 256 codes;\n",
    "         11111111 reserved for special cases → 255 codes;\n",
    "         one code (00000000) for zero → 254 codes; \n",
    "         need both positive and negative exponents → half positives (127),\n",
    "         and half negatives (127)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The exp field represents the exponent as a biased number.\n",
    "\n",
    "– It contains the actual exponent plus 127 for single precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example:\n",
    "    \n",
    "    exp = 4 => 4-127 = -123\n",
    "\n",
    "    exp = 93 => 93 - 127 = 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/bin_to_dec.jpg\" width=\"480\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Because truly mantissa looks like 1.11000000000...\n",
    "\n",
    "that's why we use (1+0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Special values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/Special.png\" width=\"240\" height=\"240\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/Range.jpg\" width=\"360\" height=\"360\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/Roundoff.jpg\" width=\"360\" height=\"360\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33554432.]\n",
      "[33554431]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([33554431], dtype=np.float32)) # some integers can't pe represented by float\n",
    "print(np.array([33554431], dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1]\n",
      "[0.3]\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(np.array([.10], dtype=np.float32))\n",
    "#Some simple decimal numbers cannot be represented exactly in binary to begin with\n",
    "# 0.10 base(10)= 0.0001100110011...base(2)\n",
    "print(np.array([0.1], dtype=np.float32)*np.array([3]))\n",
    "print(1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To get a feel for floating-point operations, we’ll do an addition example.\n",
    "\n",
    "    –To keep it simple, we’ll use base 10 scientific notation\n",
    "    –Assume the mantissa has fourdigits, and the exponent has one digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "99.99 + 0.161 = 100.151\n",
    "\n",
    "When normalized:\n",
    "        \n",
    "        9.999*10^1 \n",
    "        1.610^10^-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then 1.610 * 10^-1 = 0.01610 * 10^1 for addition\n",
    "    \n",
    "    4 significant digits it is getting\n",
    "\n",
    "    0.016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>This can result in a loss of least significant digits—the rightmost 1 in this case. But rewriting the number with the larger exponent could result in loss of the mostsignificant digits, which is much worse.</b>\n",
    "\n",
    "    999.9 * 10^-1 = 99.90^-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/Add.jpg\" width=\"360\" height=\"360\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/Steps35.jpg\" width=\"360\" height=\"360\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/Example_.jpg\" width=\"360\" height=\"360\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/Epsilon.jpg\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.] [2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1.0000_000596], dtype=np.float32)\n",
    "b = np.array([1.0000_0000], dtype=np.float32)\n",
    "print(a,a+b)\n",
    "a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1.0000_000597], dtype=np.float32)\n",
    "b = np.array([1.0000_000000], dtype=np.float32)\n",
    "a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0000002], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1.0000_001], dtype=np.float32)\n",
    "b = np.array([1.0000_001], dtype=np.float32)\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.], dtype=float32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1.0000_0001], dtype=np.float32)\n",
    "b = np.array([1.0000_0010], dtype=np.float32)\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "# Double\n",
    "a = np.array([1.00000_00000_00000_1], dtype=np.float64)\n",
    "b = np.array([1.00000_00000_00000_0], dtype=np.float64)\n",
    "print(a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Double\n",
    "a = np.array([1.00000_00000_00003], dtype=np.float64)\n",
    "b = np.array([1.00000_00000_00001], dtype=np.float64)\n",
    "print(\"%.15f\" % (a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000000000008\n"
     ]
    }
   ],
   "source": [
    "# Double\n",
    "a = np.array([4.00000_00000_00000], dtype=np.float64)\n",
    "b = np.array([0.00000_00000_00002], dtype=np.float64)\n",
    "print(\"%.15f\" % (a*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30000000000000004"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1+0.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
